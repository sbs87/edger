Practice with edgeR
========================================================

```{r} 
rm(list=ls())
source("http://bioconductor.org/biocLite.R")
source("http://stevenbsmith.net/source/load_R_enviornment_vars.R")
#biocLite("edgeR")
library("edgeR")
#edgeRUsersGuide()
setwd(paste("/Users/",user,"/bin/Qualifiers/edgeR",sep=""))
datafile =system.file( "extdata/pasilla_gene_counts.tsv", package="pasilla" )
pasillaCountTable =read.table( datafile, header=TRUE, row.names="gene_id" )
head(pasillaCountTable)

pasillaDesign = data.frame(
  row.names = colnames( pasillaCountTable ),
  condition = c( "untreated", "untreated", "untreated",
                 "untreated", "treated", "treated", "treated" ),
  libType = c( "single-end", "single-end", "paired-end",
               "paired-end", "single-end", "paired-end", "paired-end" ) )
pasillaDesign

pairedSamples = pasillaDesign$libType == "paired-end"
countTable = pasillaCountTable[ , pairedSamples ]
condition = pasillaDesign$condition[ pairedSamples ]

head(countTable)

group<-factor(c(1,1,2,2))
```
The following is the bare minimum needed to compare between two groups. Each function will be dissected in turn. Also, will need to compare the results to DESeq at some point.

```{r}


y <- DGEList(counts=countTable,group=group)
y <- calcNormFactors(y)
y <- estimateCommonDisp(y)
y <- estimateTagwiseDisp(y)
et <- exactTest(y)
topTags(et)
```
#### Let's plat around with some very fake data:

### Generate a gene table that contains 3 samples in each of 2 groups, with 25 genes. 
```{r}
dummy.data<-matrix(nrow=25,ncol=6)
dummy.data<-data.frame(dummy.data)
names(dummy.data)<-c("sample1.1","sample1.2","sample1.3","sample2.1","sample2.2","sample2.3")
for(i in 1:20){
dummy.data[i,]<-rnbinom(6,size=1/0.01,mu=500)
}

for(i in 21:25){
dummy.data[i,1:3]<-rnbinom(3,size=1/0.01,mu=50)
dummy.data[i,4:6]<-rnbinom(3,size=1/0.01,mu=100)
}
dummy.group<-c(1,1,1,2,2,2)
dummy.data.DE <- DGEList(counts=dummy.data,group=dummy.group)
dummy.data.DE <- calcNormFactors(dummy.data.DE)
dummy.data.DE <- estimateCommonDisp(dummy.data.DE)
dummy.data.DE <- estimateTagwiseDisp(dummy.data.DE)
dummy.data.exactTest <- exactTest(dummy.data.DE)
write.table(dummy.data.DE$tagwise.dispersion,file="dummy_data/tagwise.dispersion.txt")
write.table(dummy.data.DE$counts,file="dummy_data/counts.txt",sep="\t")
write.table(dummy.data.exactTest$table,file="dummy_data/exactTest.txt",sep="\t")
topTags(dummy.data.exactTest)
write.ftable(exactTest,file="dummy_data/exactTest_function.txt")
exactTestDoubleTail
log(mean(882.41550,596.9550,817.00382)/mean(541.27680 ,811.4242,678.95234))
```
#### Calc norm factors:

Let's look under the hood of calcNormFactors:

```{r}
calcNormFactors
object<-DGEList(counts=countTable,group=group)
#Defaults:
method = c("TMM", "RLE", "upperquartile", "none")
refColumn = NULL
logratioTrim = 0.3
sumTrim = 0.05
doWeighting = TRUE
Acutoff = -1e+10
p = 0.75
```
The first few lines process/clean and QC the input variables. 
Checks to make sure that the data is  DGEList object, then converts the counts into matrix form. Stores library size, as calculated during he DEGList conversion. The lib size appears to be just the colSums of counts. Check this:
colSums: `r colSums(countTable)`
lib.size: `r object$samples$lib.size`
Also note that if the lib size wasnt calculated using the DGEList method, it does it here
```{r}
if (is(object, "DGEList")) {
        x <- as.matrix(object$counts)
        lib.size <- object$samples$lib.size
    }else {
        x <- as.matrix(object)
        lib.size <- colSums(x)
    }
```
The first line of the following code gets executed implicity during a function call, but had to be modified for the purposes here. The second rgument, choices=method, is not in the function but this is what happens implicilt within the function call. The match.arg simply matches a given method to a list of potential choices. In this case, it takes the first element of method (4 elemtns) matches to the first (TMM) and assigns the signle element TMM as the method variable. The choices=method statement is the default vals of method in the function declaration. Confusing.  

The next lines:
* calcualte the rows (genes) that have zero counts across all samples and keep only the ones with at least one non zero gene count
* then, if there are no genes left or only one sample, set normalization method to none
* execute normaliztion method depending on the value of method variable (switch statement). Store the resulting normlizaton as f. Dig into the TMM and other normalization methods later. 
* finally, dependning on wether the inital input was a DEGobject, return the normalizaion factors either as a vector or within the original object as object$samples$norm.factors (return the whole updated object to the original calcNormFactors call)

The actual calls to each method seem to be hidden or private. will need to look into how to access them.. i.e., .calcFactorQuantile
```{r}
    method <- match.arg(method,choices=method)
    allzero <- rowSums(x > 0) == 0
    if (any(allzero)) x <- x[!allzero, , drop = FALSE]
    if (nrow(x) == 0 || ncol(x) == 1) method = "none"
#Depending on method...
    f <- switch(method, 
      TMM = {
        f75 <- .calcFactorQuantile(data = x, lib.size = lib.size,p = 0.75)
        if (is.null(refColumn)) refColumn <- which.min(abs(f75 -mean(f75)))
        if (length(refColumn) == 0 | refColumn < 1 | refColumn > ncol(x)) refColumn <- 1
        f <- rep(NA, ncol(x))
        for (i in 1:ncol(x)) f[i] <- .calcFactorWeighted(obs = x[, 
            i], ref = x[, refColumn], libsize.obs = lib.size[i], 
            libsize.ref = lib.size[refColumn], logratioTrim = logratioTrim, 
            sumTrim = sumTrim, doWeighting = doWeighting, Acutoff = Acutoff)
        f
    }, 
      RLE = .calcFactorRLE(x)/lib.size, 
      upperquartile = .calcFactorQuantile(x,lib.size, p = p), 
      none = rep(1, ncol(x))
    ) ## end of switch statment
f <- f/exp(mean(log(f))) #what is this?

## Depending on object type, return within the object or sumply as a vector. 
    if (is(object, "DGEList")) {
        object$samples$norm.factors <- f
        return(object)
    }else {
        return(f)
    }
```

#### Calc norm factors:

#### estimateCommonDisp:
estimateCommonDisp

```{r}
object = y
tol = 1e-06
rowsum.filter = 5
verbose = FALSE

    group <- object$samples$group <- as.factor(object$samples$group) ## group= 1,1,2,2 ... as factors
    if (all(tabulate(group) <= 1)) { ## ensures there are at least 2 samples in at least 1 group (tabulate counts the number of times each intenger occurs. Note that one group is allowed to have only 1 replicate, but not all). Exits entire dispersion estimation scwipt and assigns common dispersion as NA. 
        warning("There is no replication, setting dispersion to NA.")
        object$common.dispersion <- NA
        return(object)
    }

    tags.used <- rowSums(object$counts) > rowsum.filter ## Ensures that there are at least 5 observed counts amonst all samples
    pseudo.obj <- object[tags.used, ] ## Only uses counts passing filter. Note that this may help to curb biases due to zero inflation
    disp <- 0.01
    for (i in 1:2) { ## do the following twice, although i dont see where i gets used in the looop. 
        out <- equalizeLibSizes(object, dispersion = disp) ## black box. generates comprarable sample counts using all counts. USE DISPERSION ESTIMATE-> this changes depending on the iteration of for loop. 
        pseudo.obj$counts <- out$pseudo.counts[tags.used, , drop = FALSE] ## filter on tags only
        y <- splitIntoGroups(pseudo.obj) ## split into groupes depending on design structure
        delta <- optimize(commonCondLogLikDerDelta, interval = c(1e-04,100/(100 + 1)), tol = tol, maximum = TRUE, y = y, der = 0) ## heavy lifting. Optimize is really a wrapper to a C/C++ function called C_do_fmin. The function is passed without argument list, and the code knows to pass args to the function, internalized known as f, to the list. This is def a balck box, but my guess it that it finds the max/min emperically by plugging in vals from 1E-4 to .99 by increaments of 1e-6. 
        #THe returned val is a list. First val is either max or min, second is teh value of the function at the optimized value (undifferentiated). For example, 
        dummy_f<-function(x){
          return(3*x^2+x) ## min occurs at x=-1/6, where d/dt=0. Max is where ever end interval is. The value of the function evaluated at x=-1/6 which is -0.0833

        }
        optimize(dummy_f,interval=c(-2,2),maximum=F)
        
        delta <- delta$maximum
        disp <- delta/(1 - delta) # changes depending on interation of for loop
    }
    if (verbose) 
        cat("Disp =", round(disp, 5), ", BCV =", round(sqrt(disp), 
            4), "\n")
    object$common.dispersion <- disp
    object$pseudo.counts <- out$pseudo.counts
    object$AveLogCPM <- aveLogCPM(object)
    object$pseudo.lib.size <- out$common.lib.size
    object

```
#### Inside optimize
```{r}
f=commonCondLogLikDerDelta
interval= c(1e-04,100/(100 + 1))
#...
y = y
der=0
#...
maximum = TRUE
tol =tol ##1e-06
optimize
val <- .External2(C_do_fmin, function(arg) -f(arg, ...), lower, upper, tol)
list(maximum = val, objective = f(val, ...))

```
#### Inside the commonCondLogLikDerDelta(y,delta) function
Realize that this is being called iteratvley by optimize (?) over an interval y=0.01..0.99. What I don't get is how y counts and y interval are seperated. To test this, optimize the commonCondLogLikDerDelta function over the interval WITHOUT definining y first. Also, the delta doesnt need to be defined byecause the optimize code doesnt pass these arguments on through. weird. 
```{r}
 # note that y is the first argument of the function, and so when called by optimize, searches for the maximum over the interval 0.01 to .99 wrt to y. 
## THink of this function as 3*x^2+x, where there exists a max value over a set interval (the first derivative). This function is condidition on a particualr value of the input value y, but optimize is feeding it values over 0.01...0.99. 

        l0 <- 0
der=0
    for (i in 1:length(y)) { ## for each group in the counts
        l0 <- condLogLikDerDelta(y[[i]], delta, der = der) + l0 #sum each group's condLogLikDerDelta. Notice the value of delta was passed in from each iteration of the optimize script. The output is a per gene log liklihood (I think.. drill down into what condLogLikDerDelta does)
    }
    sum(l0) ## then sum these together to give an overall log like
    #####
```
#### Inside the ?condLogLikDerDelta(y,delta) function
?optimize
commonCondLogLikDerDelta
condLogLikDerDelta
```{r} 
r <- (1/delta) - 1
switch(der + 1L, condLogLikDerSize(y, r, der = 0L), 
      condLogLikDerSize(y,r, der = 1L) * (-delta^(-2)), 
      condLogLikDerSize(y, r, der = 1L) * 2 * (delta^(-3)) + condLogLikDerSize(y, r,der = 2) * (delta^(-4))) ## Will evaluate to the first argument, condLogLikDerSize(y, r, der = 0L),  since der=0
```
#### Inside the condLogLikDerSize(y, r, der=1L) function
```{r}
der = 1L

    if (is.vector(y)) { ## what is y here???
        y <- matrix(y, nrow = 1)
    }else {
        y <- as.matrix(y)
    }
    n <- ncol(y) ## Number of ...
    m <- rowMeans(y) ## mean values of each tag
    switch(der + 1L, 
           rowSums(lgamma(y + r)) + lgamma(n * r) - lgamma(n * (m + r)) - n * lgamma(r), 
           rowSum(digamma(y + r)) + n * digamma(n * r) - n * digamma(n * (m + r)) - n * digamma(r),
           rowSums(trigamma(y + r)) + n^2 * trigamma(n * r) - n^2 * trigamma(n * (m + r)) - n *trigamma(r))

```
#### estimateTagwiseDisp:
#### exactTest:
```{r}
pair = 1:2
dispersion = "auto"
rejection.region = "doubletail"
big.count = 900
prior.count = 0.125
```
```{r}
    if (!is(object, "DGEList")) 
        stop("Currently only supports DGEList objects as the object argument.")
    if (length(pair) != 2) 
        stop("Pair must be of length 2.")
    rejection.region <- match.arg(rejection.region, c("doubletail", 
        "deviance", "smallp"))
    group <- as.factor(object$samples$group)
    levs.group <- levels(group)
    if (is.numeric(pair)) pair <- levs.group[pair] else pair <- as.character(pair)
    if (!all(pair %in% levs.group)) stop("At least one element of given pair is not a group.\n Groups are: ",paste(levs.group, collapse = " "))
    if (is.null(dispersion)) 
        dispersion <- "auto"
    if (is.character(dispersion)) {
        dispersion <- match.arg(dispersion, c("auto", "common","trended", "tagwise"))
        dispersion <- switch(dispersion, common = object$common.dispersion, 
            trended = object$trended.dispersion, tagwise = object$tagwise.dispersion, 
            auto = getDispersion(object))
        if (is.null(dispersion)) 
            stop("specified dispersion not found in object")
        if (is.na(dispersion[1])) 
            stop("dispersion is NA")
    }
    ldisp <- length(dispersion)
    ntags <- nrow(object$counts)
    if (ldisp != 1 && ldisp != ntags) 
        stop("Dispersion provided by user must have length either 1 or the number of tags in the DGEList object.")
    if (ldisp == 1) 
        dispersion <- rep(dispersion, ntags)
```
Beggining of the real meat:
* First determine the group to include in the calculation and keep that as y
* Store the library size and normalization factors for that group as respecive vars.
```{r}
group <- as.character(group)
    j <- group %in% pair
    y <- object$counts[, j, drop = FALSE]
    lib.size <- object$samples$lib.size[j]
    norm.factors <- object$samplesamples$norm.factors[j]
    group <- group[j]
    if (is.null(rownames(y))) 
        rownames(y) <- paste("tag", 1:ntags, sep = ".")
```
* Adjust library size by normalization factors
* Use this adjusted lib size as offset
* Calculate prior count and offset aug... *look into what theyre doing here*.
```{r}
    lib.size <- lib.size * norm.factors
    offset <- log(lib.size)
    lib.size.average <- exp(mean(offset))
    prior.count <- prior.count * lib.size/mean(lib.size)
    offset.aug <- log(lib.size + 2 * prior.count)
    j1 <- group == pair[1]
    n1 <- sum(j1)
    if (n1 == 0) 
        stop("No libraries for", pair[1])
```
* Calculate abundance? (by some mysterious other function mglmOneGroup, see below)
```{r}
    y1 <- y[, j1, drop = FALSE]
    abundance1 <- mglmOneGroup(y1 + matrix(prior.count[j1], ntags, 
        n1, byrow = TRUE), offset = offset.aug[j1], dispersion = dispersion)
    j2 <- group == pair[2]
    n2 <- sum(j2)
    if (n1 == 0) 
        stop("No libraries for", pair[2])
    y2 <- y[, j2, drop = FALSE]
    abundance2 <- mglmOneGroup(y2 + matrix(prior.count[j2], ntags, 
        n2, byrow = TRUE), offset = offset.aug[j2], dispersion = dispersion)
    logFC <- (abundance2 - abundance1)/log(2)
    abundance <- mglmOneGroup(y, dispersion = dispersion, offset = offset)
    e <- exp(abundance)
    input.mean <- matrix(e, ntags, n1)
    output.mean <- input.mean * lib.size.average
    input.mean <- t(t(input.mean) * lib.size[j1])
    y1 <- q2qnbinom(y1, input.mean = input.mean, output.mean = output.mean, 
        dispersion = dispersion)
    input.mean <- matrix(e, ntags, n2)
    output.mean <- input.mean * lib.size.average
    input.mean <- t(t(input.mean) * lib.size[j2])
    y2 <- q2qnbinom(y2, input.mean = input.mean, output.mean = output.mean, 
        dispersion = dispersion)
```
P values calcualted using one of several methods. Default is doubletail (inside switch statement) via the exactTestDoubleTail function. Need to look into this. 
```{r}
    exact.pvals <- switch(rejection.region, doubletail = exactTestDoubleTail(y1, 
        y2, dispersion = dispersion, big.count = big.count), 
        deviance = exactTestByDeviance(y1, y2, dispersion = dispersion), 
        smallp = exactTestBySmallP(y1, y2, dispersion = dispersion))
    AveLogCPM <- object$AveLogCPM
    if (is.null(AveLogCPM)) 
        AveLogCPM <- aveLogCPM(object)
    de.out <- data.frame(logFC = logFC, logCPM = AveLogCPM, PValue = exact.pvals)
    rn <- rownames(object$counts)
    if (!is.null(rn)) 
        rownames(de.out) <- make.unique(rn)
    new("DGEExact", list(table = de.out, comparison = pair, genes = object$genes))
}
```
The exactTestDoubleTail:
```{r}
#given y1 and y2,
dispersion = 0
big.count = 900 

    ntags <- NROW(y1)
    n1 <- NCOL(y1)
    n2 <- NCOL(y2)
    if (n1 > 1) 
        s1 <- round(rowSums(y1))
    else s1 <- round(y1)
    if (n2 > 1) 
        s2 <- round(rowSums(y2))
    else s2 <- round(y2)
    if (length(dispersion) == 1) 
        dispersion <- rep(dispersion, ntags)
    s <- s1 + s2
    mu <- s/(n1 + n2)
    mu1 <- n1 * mu
    mu2 <- n2 * mu
    pvals <- rep(1, ntags)
    names(pvals) <- names(y1)
    pois <- dispersion <= 0
    if (any(pois)) 
        pvals[pois] <- binomTest(s1[pois], s2[pois], p = n1/(n1 + 
            n2))
    big <- s1 > big.count & s2 > big.count
    if (any(big)) {
        y1 <- as.matrix(y1)
        y2 <- as.matrix(y2)
        pvals[big] <- exactTestBetaApprox(y1[big, , drop = FALSE], 
            y2[big, , drop = FALSE], dispersion[big])
    }
    p.bot <- size1 <- size2 <- rep(0, ntags)
    left <- s1 < mu1 & !pois & !big
    if (any(left)) {
        p.bot[left] <- dnbinom(s[left], size = (n1 + n2)/dispersion[left], 
            mu = s[left])
        size1[left] <- n1/dispersion[left]
        size2[left] <- n2/dispersion[left]
        for (g in which(left)) {
            x <- 0:s1[g]
            p.top <- dnbinom(x, size = size1[g], mu = mu1[g]) * 
                dnbinom(s[g] - x, size = size2[g], mu = mu2[g])
            pvals[g] <- 2 * sum(p.top)
        }
        pvals[left] <- pvals[left]/p.bot[left]
    }
    right <- s1 > mu1 & !pois & !big
    if (any(right)) {
        p.bot[right] <- dnbinom(s[right], size = (n1 + n2)/dispersion[right], 
            mu = s[right])
        size1[right] <- n1/dispersion[right]
        size2[right] <- n2/dispersion[right]
        for (g in which(right)) {
            x <- s1[g]:s[g]
            p.top <- dnbinom(x, size = size1[g], mu = mu1[g]) * 
                dnbinom(s[g] - x, size = size2[g], mu = mu2[g])
            pvals[g] <- 2 * sum(p.top)
        }
        pvals[right] <- pvals[right]/p.bot[right]
    }
    pmin(pvals, 1)
}
```
#### topTags:



